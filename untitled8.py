# -*- coding: utf-8 -*-
"""Untitled8.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1f49yzZaE5fbkl40boB1CxNWuGb5Hxz9M

### 3. Programming Task (Basic GAN Implementation)
"""

import torch
import torch.nn as nn
import torch.optim as optim
from torchvision import datasets, transforms
from torch.utils.data import DataLoader
import matplotlib.pyplot as plt
import numpy as np

# Define hyperparameters
latent_dim = 100
image_size = 784  # 28x28
batch_size = 128
epochs = 101
learning_rate = 0.0002
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# Load MNIST dataset
transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])
train_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)
train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)

# Generator Architecture
class Generator(nn.Module):
    def __init__(self):
        super(Generator, self).__init__()
        self.main = nn.Sequential(
            nn.Linear(latent_dim, 256),
            nn.LeakyReLU(0.2, inplace=True),
            nn.Linear(256, 512),
            nn.LeakyReLU(0.2, inplace=True),
            nn.Linear(512, 1024),
            nn.LeakyReLU(0.2, inplace=True),
            nn.Linear(1024, image_size),
            nn.Tanh()  # Output in [-1, 1] to match normalized MNIST
        )

    def forward(self, input):
        return self.main(input)

# Discriminator Architecture
class Discriminator(nn.Module):
    def __init__(self):
        super(Discriminator, self).__init__()
        self.main = nn.Sequential(
            nn.Linear(image_size, 512),
            nn.LeakyReLU(0.2, inplace=True),
            nn.Linear(512, 256),
            nn.LeakyReLU(0.2, inplace=True),
            nn.Linear(256, 1),
            nn.Sigmoid()  # Output probability (real or fake)
        )

    def forward(self, input):
        return self.main(input)

# Initialize generator and discriminator
generator = Generator().to(device)
discriminator = Discriminator().to(device)

# Loss function and optimizers
criterion = nn.BCELoss()
optimizer_g = optim.Adam(generator.parameters(), lr=learning_rate)
optimizer_d = optim.Adam(discriminator.parameters(), lr=learning_rate)

# Fixed noise for visualization
fixed_noise = torch.randn(64, latent_dim, device=device)

# Lists to store losses
generator_losses = []
discriminator_losses = []

# Training loop
for epoch in range(epochs):
    for i, (real_images, _) in enumerate(train_loader):
        real_images = real_images.view(-1, image_size).to(device)
        batch_size = real_images.size(0)
        real_labels = torch.ones(batch_size, 1).to(device)
        fake_labels = torch.zeros(batch_size, 1).to(device)

        # --- Train Discriminator ---
        optimizer_d.zero_grad()

        # Real images loss
        output_real = discriminator(real_images)
        loss_real = criterion(output_real, real_labels)

        # Fake images loss
        noise = torch.randn(batch_size, latent_dim, device=device)
        fake_images = generator(noise)
        output_fake = discriminator(fake_images.detach())  # Detach to avoid generator gradients
        loss_fake = criterion(output_fake, fake_labels)

        # Total discriminator loss
        loss_discriminator = loss_real + loss_fake
        loss_discriminator.backward()
        optimizer_d.step()

        # --- Train Generator ---
        optimizer_g.zero_grad()

        # Generate fake images
        noise = torch.randn(batch_size, latent_dim, device=device)
        fake_images = generator(noise)
        output_fake_g = discriminator(fake_images)  # Train generator to fool discriminator

        # Generator loss (aims for discriminator to classify fake as real)
        loss_generator = criterion(output_fake_g, real_labels)
        loss_generator.backward()
        optimizer_g.step()

        # Store losses
        generator_losses.append(loss_generator.item())
        discriminator_losses.append(loss_discriminator.item())

        # Print progress
        if (i + 1) % 200 == 0:
            print(f"Epoch [{epoch}/{epochs}] Batch [{i+1}/{len(train_loader)}] "
                  f"D Loss: {loss_discriminator.item():.4f}, G Loss: {loss_generator.item():.4f}")

    # Generate and save sample images
    if epoch == 0 or epoch == 50 or epoch == 100:
        generator.eval()
        with torch.no_grad():
            fake_samples = generator(fixed_noise).cpu().view(-1, 28, 28)
            fig, axes = plt.subplots(8, 8, figsize=(8, 8))
            for i, ax in enumerate(axes.flatten()):
                ax.imshow(fake_samples[i].numpy(), cmap='gray')
                ax.axis('off')
            plt.suptitle(f"Epoch: {epoch}")
            plt.tight_layout()
            plt.savefig(f"generated_epoch_{epoch}.png")
            plt.close()
        generator.train()

# Plotting losses
plt.figure(figsize=(10, 5))
plt.plot(generator_losses, label='Generator Loss')
plt.plot(discriminator_losses, label='Discriminator Loss')
plt.xlabel('Iteration')
plt.ylabel('Loss')
plt.title('Generator and Discriminator Losses During Training')
plt.legend()
plt.savefig("loss_plot.png")
plt.show()

"""

## 4. Programming Task (Data Poisoning Simulation)"""

import numpy as np
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, confusion_matrix, ConfusionMatrixDisplay

# 1. Create a small, synthetic dataset of movie reviews
reviews = [
    "This movie was fantastic! The acting was superb.",
    "I really enjoyed the plot and the characters.",
    "A truly terrible film. Waste of my time.",
    "The worst movie I've seen in years. Avoid!",
    "It was okay, nothing particularly special.",
    "The special effects were amazing and the story was engaging.",
    "UC Berkeley is a great place for studies.",
    "I hated every single minute of this boring movie.",
    "UC Berkeley has a beautiful campus.",
    "A delightful and heartwarming story.",
    "The direction was poor and the script was weak.",
    "UC Berkeley offers excellent research opportunities.",
    "This film is a masterpiece. Absolutely loved it!",
    "Could not finish it. So bad.",
    "UC Berkeley's football team needs improvement.",
]
labels = np.array([1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0])  # 1 for positive, 0 for negative

# Identify indices related to "UC Berkeley"
berkeley_indices = [i for i, review in enumerate(reviews) if "UC Berkeley" in review]

# Split data into training and testing sets
train_reviews, test_reviews, train_labels, test_labels = train_test_split(
    reviews, labels, test_size=0.3, random_state=42
)

# 2. Train a basic sentiment classifier (Logistic Regression with TF-IDF)
vectorizer = TfidfVectorizer()
X_train = vectorizer.fit_transform(train_reviews)
X_test = vectorizer.transform(test_reviews)

classifier = LogisticRegression(random_state=42)
classifier.fit(X_train, train_labels)

# Evaluate before poisoning
predictions_before = classifier.predict(X_test)
accuracy_before = accuracy_score(test_labels, predictions_before)
cm_before = confusion_matrix(test_labels, predictions_before)

print(f"Accuracy before poisoning: {accuracy_before:.4f}")
print("Confusion Matrix before poisoning:")
print(cm_before)

# Plot confusion matrix before poisoning
disp_before = ConfusionMatrixDisplay(confusion_matrix=cm_before, display_labels=['Negative', 'Positive'])
disp_before.plot(cmap=plt.cm.Blues)
plt.title('Confusion Matrix Before Poisoning')
plt.savefig('confusion_matrix_before.png')
plt.show()

# 3. Poison the training data by flipping labels for "UC Berkeley" phrases
poisoned_train_reviews = list(train_reviews)
poisoned_train_labels = np.array(train_labels)

for i in range(len(train_reviews)):
    if "UC Berkeley" in train_reviews[i]:
        poisoned_train_labels[i] = 1 - poisoned_train_labels[i]  # Flip the label

print("\nPoisoning complete.")

# 4. Train the classifier on the poisoned data
poisoned_vectorizer = TfidfVectorizer()
X_train_poisoned = poisoned_vectorizer.fit_transform(poisoned_train_reviews)
X_test_poisoned = poisoned_vectorizer.transform(test_reviews)  # Use original test set

poisoned_classifier = LogisticRegression(random_state=42)
poisoned_classifier.fit(X_train_poisoned, poisoned_train_labels)

# Evaluate after poisoning
predictions_after = poisoned_classifier.predict(X_test_poisoned)
accuracy_after = accuracy_score(test_labels, predictions_after)
cm_after = confusion_matrix(test_labels, predictions_after)

print(f"Accuracy after poisoning: {accuracy_after:.4f}")
print("Confusion Matrix after poisoning:")
print(cm_after)

# Plot confusion matrix after poisoning
disp_after = ConfusionMatrixDisplay(confusion_matrix=cm_after, display_labels=['Negative', 'Positive'])
disp_after.plot(cmap=plt.cm.Blues)
plt.title('Confusion Matrix After Poisoning')
plt.savefig('confusion_matrix_after.png')
plt.show()

# 5. Plot accuracy before and after
labels_plot = ['Before Poisoning', 'After Poisoning']
accuracies = [accuracy_before, accuracy_after]

plt.figure(figsize=(6, 4))
plt.bar(labels_plot, accuracies, color=['skyblue', 'salmon'])
plt.ylabel('Accuracy')
plt.title('Accuracy Before and After Data Poisoning')
plt.ylim([0, 1])
plt.savefig('accuracy_comparison.png')
plt.show()